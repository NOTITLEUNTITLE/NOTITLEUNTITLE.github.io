---
layout: single
title:  "[study] NLP 공부[22-01-24 업데이트]"
categories: study
tag: [pytorch, python]
toc: true
author_profile: false
---

# Flow
<ol>
<li>Bag-of-words vector</li>
<li>Word Embedding</li>
<li>CoVc</li>
<li>GPT-1</li>
<li>BERT, GPT-2</li>
<li>XLM(번역)</li>
<li>XLNet(transformer-XL)</li>
<li>RoBERTa</li>
<li>ELECTRA(GAN)(replace, original)</li>
<li>ALBERT(빠른 BERT, SOP)</li>
<li>DistillBERT, TinyBERT</li>
<li>T5</li>
<li>GPT-3</li>
</ol>
<br/>
<br/>
위에 나열되 있는 개념들을 정리할 예정이다.<br/>
(<strong>논문</strong> 리뷰, example code, pytorch 개념 등등)<br/><br/><br/>

# 필요한 개념 및 용어정리.

## 들어가기에 앞서(Transformer)
Transformer에 대해서 이야기를 안 꺼낼수가 없다.<br/>
Flow에서 나열하지 않은모델과 개념들이 많지만, 저기 위에있는거 다 몰라도, Transformer만 알아도 된다.<br/>
Transformer에 대해서 제일 먼저 개념정리를 하는것이 중요한것 같다.(2022-01-24 시점에는 이렇지만, 다른 시간에는 어떨지 모르겠다..)<br/>

- query, key, value
<p>key는 value를 가리키는 요소입니다. 
하나의 key에 하나의 value만 매칭됨으로써, key가 특정되면 해당하는 key에 맞는 value를 얻을 수 있습니다.
query는 원하는 key가 어떤 것인지를 나타내는 요소입니다.
query에 적합한 혹은 query와 일치하는 key와 매칭되는 value들을 얻을 때 사용합니다.</p>

<p>Transformer attention에서는 query와 key의 dot product 를 통해 둘 간의 관계성을 얻어내고,
softmax function을 통해 soft한 형태의 attention score를 얻어내게 됩니다.
그 후, attention score와 value를 dot product하여 query와 가장 잘 매칭되는 key를 가진 value들을 가져옵니다.</p>


- multi-head

multi-head는 single-head에 비해 다양한 관점으로 문장을 분석할 수 있게 해줍니다.<br/>
예시 링크: <a href="https://i.imgur.com/HjYb7F2.png">https://i.imgur.com/HjYb7F2.png</a>

- Transformer embedding과 BERT embedding의 차이점.
<p>BERT embedding 에는 segment embedding이 있어서, 두 문장을 구분할 수 있게 해줍니다.</p>
<p>BERT embedding은 learnable parameter를 통해 positional encoding을 하고, Transformer는 sinusoidal function을 통해 positional encoding을 적용합니다.</p>

---위의 내용은 김태성님의 자료입니다.---



# Bag-of-word vector
