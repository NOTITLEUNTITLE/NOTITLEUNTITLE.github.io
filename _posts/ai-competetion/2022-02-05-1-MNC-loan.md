---
layout: single
title:  "[MNC] 대출자 채무 불이행 여부 예측 모델"
categories: ML
tag: [python, pytorch, ML]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>



```python
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

import xgboost
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, RidgeCV, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier

import sklearn.metrics as metrics
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix
from tqdm import tqdm

import warnings
warnings.filterwarnings('ignore')

```


```python
from google.colab import drive
drive.mount('/content/drive')
```

<pre>
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
</pre>
# EDA


## 변수 설명

  - **int_rate** : 대출자에 부여된 이자율 (Interest rate of the loan the applicant received)

  - **annual_inc** : 연 소득 (annual income)

  - **dti** : 소득 대비 부채 비율 (Debt-to-income ratio)

  - **delinq_2yrs** : 지난 2년 간 체납 발생 횟수 (Delinquencies on lines of credit in the last 2 years)

  - **inq_last_6mths** : 지난 6개월 간 신용 조회 수 (Inquiries into the applicant's credit during the last 6 months)

  - **pub_rec** : 파산 횟수 (Number of bankruptcies listed in the public record)

  - **revol_bal** : 리볼빙 잔액 (Total credit revolving balance)

  - **total_acc** : 지금까지 소유했던 신용카드 개수 (num_total_cc_accounts : Total number of credit card accounts in the applicant's history)

  - **collections_12_mths_ex_med** : 의료부문을 제외한 지난 12개월 간 추심 발생 횟수 (num_collections_last_12m : Number of collections in the last 12 months. This excludes medical collections)

  - **acc_now_delinq** : 대출자가 체납 상태에 있지 않은 계좌의 수 (The number of accounts on which the borrower is now delinquent)

  - **tot_coll_amt** : 대출자에 대한 현재까지의 총 추심액 (total_collection_amount_ever : The total amount that the applicant has had against them in collections)

  - **tot_cur_bal** : 전 계좌의 현재 통합 잔고 (Total current balance of all accounts)

  - **chargeoff_within_12_mths** : 대출 부 신청인의 대출 신청 직전 12개월 간 세금 공제 횟수 (Number of charge-offs within last 12 months at time of application for the secondary applicant)

  - **delinq_amnt** : 체납 금액 (delinquency amount)

  - **tax_liens** : 세금 저당권의 수 (Number of tax liens)

  - **emp_length1 ~ 12** : 고용 연수 (Number of years in the job)

  - **home_ownership1 ~ 6** : 대출 신청자의 주거 소유 형태 (The ownership status of the applicant's residence)

  - **verification_status1 ~ 3** : 공동 소득 발생 여부 및 형태 (verification_income_joint : Type of verification of the joint income)

  - **purpose1 ~ 14** : 대출 목적 (The purpose of the loan)

  - **initial_list_status1 ~ 2** : 최초 대출 상태 (Initial listing status of the loan)

  - **mths_since_last_delinq1 ~ 11** : 마지막 체납이 지금으로부터 몇개월 전에 있었는지를 나타내는 변수 (Months since the last delinquency)

  

  - **funded_amnt** : 대출액 (Funded amount)

  - **funded_amnt_inv** : 사채 대출액 (Funded amount by investors)

  - **total_rec_late_fee** : 총 연체료 중 납부액 (Late fees received to date)

  - **term1** : 상환 기간 (The number of payments on the loan. Values are in months and can be either 36 or 60)

  - **open_acc** : 개설 개좌 수 (The number of open credit lines in the borrower's credit file)

  - **installment** : 대출 발생 시 월 상환액 (The monthly payment owed by the borrower if the loan originates)

  - **revol_util** : 리볼빙 한도 대비 리볼빙 사용 비율 (Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit)

  - **out_prncp** : 대출액 중 원리금 잔액 (Remaining outstanding principal for total amount funded)

  - **out_prncp_inv** : 사채 대출액 중 원리금 잔액 (Remaining outstanding principal for total amount funded by investors)

  - **total_rec_int** : 이자 상환액 (Interest received to date)

  - **fico_range_low** : FICO(일종의 신용점수) 최저값 (The lower boundary range the borrower’s FICO at loan origination belongs to)

  - **fico_range_high** : FICO(일종의 신용점수) 최고값 (The upper boundary range the borrower’s FICO at loan origination belongs to)

  

  - **depvar** : 고객의 부도 여부 (dependent variable)



```python
df = pd.read_csv('/content/drive/MyDrive/22-01-28/train.csv')
df.head()
```


  <div id="df-e9f1e763-919e-4661-9528-5087e31552da">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
      <th>depvar</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0824</td>
      <td>21000.0</td>
      <td>29.19</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3016</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11773</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>1200.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>18</td>
      <td>37.74</td>
      <td>0.076</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>157.94</td>
      <td>765</td>
      <td>769</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1299</td>
      <td>80000.0</td>
      <td>4.82</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>5722</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21875</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8000</td>
      <td>8000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>269.52</td>
      <td>0.447</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1702.42</td>
      <td>665</td>
      <td>669</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1299</td>
      <td>38000.0</td>
      <td>23.66</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>6511</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31868</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>5000</td>
      <td>5000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>7</td>
      <td>168.45</td>
      <td>0.880</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1066.64</td>
      <td>670</td>
      <td>674</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1367</td>
      <td>100000.0</td>
      <td>16.27</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>6849</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>326049</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>15000</td>
      <td>15000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>12</td>
      <td>510.27</td>
      <td>0.457</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1256.24</td>
      <td>680</td>
      <td>684</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.1269</td>
      <td>30000.0</td>
      <td>25.28</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>8197</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>2506</td>
      <td>8840</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10000</td>
      <td>10000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>335.45</td>
      <td>0.416</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>871.04</td>
      <td>660</td>
      <td>664</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-e9f1e763-919e-4661-9528-5087e31552da')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-e9f1e763-919e-4661-9528-5087e31552da button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-e9f1e763-919e-4661-9528-5087e31552da');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  


기본적으로 MNC에서 제공한 데이터는 깔끔했다. Feature engineering도 잘했다.   

(처음에는 데이터를 받았으니, Feature engineering을 한다고, 시간을 많이 보냈는데, 잘했다...)



```python
df.describe()
```


  <div id="df-2a9d2458-251e-464a-a102-5963030eeeed">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
      <th>depvar</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100000.000000</td>
      <td>1.000000e+05</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>1.000000e+05</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.00000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.0</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.00000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>1.000000e+05</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
      <td>100000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.130833</td>
      <td>7.436061e+04</td>
      <td>18.514508</td>
      <td>0.343700</td>
      <td>0.668880</td>
      <td>0.235720</td>
      <td>16090.202820</td>
      <td>24.675910</td>
      <td>0.018810</td>
      <td>0.005800</td>
      <td>250.588120</td>
      <td>1.305281e+05</td>
      <td>0.009630</td>
      <td>19.288330</td>
      <td>0.063820</td>
      <td>0.065400</td>
      <td>0.326240</td>
      <td>0.089410</td>
      <td>0.08058</td>
      <td>0.059480</td>
      <td>0.060710</td>
      <td>0.046640</td>
      <td>0.045320</td>
      <td>0.045690</td>
      <td>0.038570</td>
      <td>0.075550</td>
      <td>0.066410</td>
      <td>0.000330</td>
      <td>0.466120</td>
      <td>0.000070</td>
      <td>0.000050</td>
      <td>0.110880</td>
      <td>0.422550</td>
      <td>0.321800</td>
      <td>0.390200</td>
      <td>0.288000</td>
      <td>0.009620</td>
      <td>0.226720</td>
      <td>0.582810</td>
      <td>0.0</td>
      <td>0.060400</td>
      <td>0.004250</td>
      <td>0.019830</td>
      <td>0.01101</td>
      <td>0.007310</td>
      <td>0.058200</td>
      <td>0.000650</td>
      <td>0.011250</td>
      <td>0.007070</td>
      <td>0.000880</td>
      <td>0.434430</td>
      <td>0.565570</td>
      <td>0.489320</td>
      <td>0.052360</td>
      <td>0.060090</td>
      <td>0.045700</td>
      <td>0.050750</td>
      <td>0.048560</td>
      <td>0.051340</td>
      <td>0.053430</td>
      <td>0.048960</td>
      <td>0.050030</td>
      <td>0.049460</td>
      <td>13735.317750</td>
      <td>13729.341073</td>
      <td>2.448885e+00</td>
      <td>0.855920</td>
      <td>11.620700</td>
      <td>434.077648</td>
      <td>0.537233</td>
      <td>0.253327</td>
      <td>0.253259</td>
      <td>2491.282802</td>
      <td>692.630550</td>
      <td>696.630660</td>
      <td>0.325690</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.044773</td>
      <td>7.467409e+04</td>
      <td>8.413049</td>
      <td>0.905007</td>
      <td>0.952044</td>
      <td>0.661468</td>
      <td>21569.939271</td>
      <td>11.883834</td>
      <td>0.150321</td>
      <td>0.083585</td>
      <td>2042.770881</td>
      <td>1.503326e+05</td>
      <td>0.110079</td>
      <td>893.304366</td>
      <td>0.468027</td>
      <td>0.247232</td>
      <td>0.468839</td>
      <td>0.285336</td>
      <td>0.27219</td>
      <td>0.236522</td>
      <td>0.238799</td>
      <td>0.210868</td>
      <td>0.208006</td>
      <td>0.208813</td>
      <td>0.192569</td>
      <td>0.264278</td>
      <td>0.248999</td>
      <td>0.018163</td>
      <td>0.498853</td>
      <td>0.008366</td>
      <td>0.007071</td>
      <td>0.313985</td>
      <td>0.493968</td>
      <td>0.467169</td>
      <td>0.487797</td>
      <td>0.452833</td>
      <td>0.097609</td>
      <td>0.418712</td>
      <td>0.493097</td>
      <td>0.0</td>
      <td>0.238228</td>
      <td>0.065054</td>
      <td>0.139417</td>
      <td>0.10435</td>
      <td>0.085186</td>
      <td>0.234122</td>
      <td>0.025487</td>
      <td>0.105468</td>
      <td>0.083786</td>
      <td>0.029652</td>
      <td>0.495684</td>
      <td>0.495684</td>
      <td>0.499888</td>
      <td>0.222753</td>
      <td>0.237655</td>
      <td>0.208835</td>
      <td>0.219488</td>
      <td>0.214947</td>
      <td>0.220691</td>
      <td>0.224891</td>
      <td>0.215786</td>
      <td>0.218008</td>
      <td>0.216828</td>
      <td>8464.825314</td>
      <td>8461.694483</td>
      <td>1.489496e+01</td>
      <td>0.351173</td>
      <td>5.458774</td>
      <td>265.921746</td>
      <td>0.239373</td>
      <td>18.053290</td>
      <td>18.051746</td>
      <td>2706.262200</td>
      <td>29.668017</td>
      <td>29.668584</td>
      <td>0.468634</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.053200</td>
      <td>5.360000e+03</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1000.000000</td>
      <td>800.000000</td>
      <td>-2.000000e-09</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>23.360000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>660.000000</td>
      <td>664.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.097500</td>
      <td>4.500000e+04</td>
      <td>12.200000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6009.000000</td>
      <td>16.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.698900e+04</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7200.000000</td>
      <td>7200.000000</td>
      <td>0.000000e+00</td>
      <td>1.000000</td>
      <td>8.000000</td>
      <td>240.292500</td>
      <td>0.361000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>857.292500</td>
      <td>670.000000</td>
      <td>674.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.127400</td>
      <td>6.200000e+04</td>
      <td>18.060000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>11030.500000</td>
      <td>23.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6.802350e+04</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>12000.000000</td>
      <td>12000.000000</td>
      <td>0.000000e+00</td>
      <td>1.000000</td>
      <td>11.000000</td>
      <td>366.370000</td>
      <td>0.541000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1615.160000</td>
      <td>685.000000</td>
      <td>689.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.158000</td>
      <td>9.000000e+04</td>
      <td>24.530000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>19540.000000</td>
      <td>31.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.943098e+05</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>19200.000000</td>
      <td>19200.000000</td>
      <td>0.000000e+00</td>
      <td>1.000000</td>
      <td>14.000000</td>
      <td>575.860000</td>
      <td>0.720000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>3039.115000</td>
      <td>705.000000</td>
      <td>709.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.309900</td>
      <td>8.300000e+06</td>
      <td>49.930000</td>
      <td>20.000000</td>
      <td>6.000000</td>
      <td>63.000000</td>
      <td>971736.000000</td>
      <td>176.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>197765.000000</td>
      <td>3.164353e+06</td>
      <td>7.000000</td>
      <td>94521.000000</td>
      <td>63.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>40000.000000</td>
      <td>40000.000000</td>
      <td>8.741700e+02</td>
      <td>1.000000</td>
      <td>82.000000</td>
      <td>1584.900000</td>
      <td>8.923000</td>
      <td>2330.970000</td>
      <td>2330.970000</td>
      <td>28005.960000</td>
      <td>845.000000</td>
      <td>850.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-2a9d2458-251e-464a-a102-5963030eeeed')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-2a9d2458-251e-464a-a102-5963030eeeed button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-2a9d2458-251e-464a-a102-5963030eeeed');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  



```python
# 결측치 확인
pd.DataFrame(df.isnull().sum()).rename(columns={0:'Non-Null Count'}).T
```


  <div id="df-5aa422f4-0edc-43f3-bfc0-0f2d27453bcb">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
      <th>depvar</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Non-Null Count</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-5aa422f4-0edc-43f3-bfc0-0f2d27453bcb')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-5aa422f4-0edc-43f3-bfc0-0f2d27453bcb button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-5aa422f4-0edc-43f3-bfc0-0f2d27453bcb');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  


Feature engineering을 잘했다고 느껴진게 여기서 드러난다. 결측값(NaN)이 한개도 없었다.   

아무래도 모델 튜닝에 신경쓰라는 의도인것 같았다.



```python
# 라벨(depvar) 비율 확인
print('y=1 ratio :', df.depvar.sum()/len(df))
```

<pre>
y=1 ratio : 0.32569
</pre>
f1 score와 y=1 ratio, 마지막으로 macro f1   

이 세 가지가 문제를 푸는데 정말 고민하게 했다... 끝에서 얘기를 하겠습니다.


# Train Valid Split


어떤 방식으로 문제를 접근했냐면, XGBClassifier라는 모델을 선정했고,   

이 모델을 학습시킬때, 총 100,000개의 데이터들을 5등분하여,   

4개씩 트레이닝 하는 방법으로 진행하였습니다.   

예를 들면,   

dataset1, dataset2, dataset3, dataset4, dataset5 이렇게 각각 2만개의 데이터를 받고,   

model1 = 1,2,3,4 (5번제외)   

model2 = 1,2,3,5 (4번제외)   

model3 = 1,2,4,5 (3번제외)   

model4 = 1,3,4,5 (2번제외)   

model5 = 2,3,4,5 (1번제외)   

   

      

이런식으로 진행하여 최종 test_dataset에서는,   

out1 = model1(test)   

out2 = model2(test)   

out3 = model3(test)   

out4 = model4(test)   

out5 = model5(test)   

제출용 csv파일에는 5개의 모델중에서 동일한 class가 3개이상이면 해당 class로 제출하는방식을 택했습니다!!!!   





(아직 못해본게 많이 있습니다. SMOTE를 통한 데이터 증식, RandomForest model을 통한 학습 등 고려할게 많았지만,    

제가 시간을 삽질하는데에 허비해버려서, 대회 마감일에 허겁지겁 학습하게 되었습니다.    

경험이 없어서, 모르는 용어가 많았네요 ㅜㅜ)   




```python
X = df.drop('depvar', axis=1)
y = df['depvar']
```


```python
X_train1 = X[:20000]
X_train2 = X[20000:40000]
X_train3 = X[40000:60000]
X_train4 = X[60000:80000]
X_train5 = X[80000:]

y_train1 = y[:20000]
y_train2 = y[20000:40000]
y_train3 = y[40000:60000]
y_train4 = y[60000:80000]
y_train5 = y[80000:]
```


```python
##########
frames = [X_train1, X_train2, X_train3, X_train4]
X_train_dataset1 = pd.concat(frames)

frames = [X_train1, X_train2, X_train3, X_train5]
X_train_dataset2 = pd.concat(frames)

frames = [X_train1, X_train2, X_train4, X_train5]
X_train_dataset3 = pd.concat(frames)

frames = [X_train1, X_train3, X_train4, X_train5]
X_train_dataset4 = pd.concat(frames)

frames = [X_train2, X_train3, X_train4, X_train5]
X_train_dataset5 = pd.concat(frames)
##########

frames = [y_train1, y_train2, y_train3, y_train4]
y_train_dataset1 = pd.concat(frames)

frames = [y_train1, y_train2, y_train3, y_train5]
y_train_dataset2 = pd.concat(frames)

frames = [y_train1, y_train2, y_train4, y_train5]
y_train_dataset3 = pd.concat(frames)

frames = [y_train1, y_train3, y_train4, y_train5]
y_train_dataset4 = pd.concat(frames)

frames = [y_train2, y_train3, y_train4, y_train5]
y_train_dataset5 = pd.concat(frames)
```


```python
# training set과 validation set의 데이터 수 확인
print(f"X_train1 = {len(X_train1)}")
print(f"X_train2 = {len(X_train2)}")
print(f"X_train3 = {len(X_train3)}")
print(f"X_train4 = {len(X_train4)}")
print(f"X_train5 = {len(X_train5)}")
print()
print(f"y_train1 = {len(y_train1)}")
print(f"y_train2 = {len(y_train2)}")
print(f"y_train3 = {len(y_train3)}")
print(f"y_train4 = {len(y_train4)}")
print(f"y_train5 = {len(y_train5)}")
print()
print(f"y_train1의 label ratio check = {y_train1.sum() / len(y_train1)}")
print(f"y_train2의 label ratio check = {y_train2.sum() / len(y_train2)}")
print(f"y_train3의 label ratio check = {y_train3.sum() / len(y_train3)}")
print(f"y_train4의 label ratio check = {y_train4.sum() / len(y_train4)}")
print(f"y_train5의 label ratio check = {y_train5.sum() / len(y_train5)}")
```

<pre>
X_train1 = 20000
X_train2 = 20000
X_train3 = 20000
X_train4 = 20000
X_train5 = 20000

y_train1 = 20000
y_train2 = 20000
y_train3 = 20000
y_train4 = 20000
y_train5 = 20000

y_train1의 label ratio check = 0.3256
y_train2의 label ratio check = 0.3226
y_train3의 label ratio check = 0.3293
y_train4의 label ratio check = 0.3329
y_train5의 label ratio check = 0.31805
</pre>
중요한게 위에서 나왔군요..     

제가 끝까지 고민하려 했지만, 한계를 느껴서, 이 포스팅을 마치고 공부하러 떠납니다...   

(정말 모르는 용어가 너무 많고, 아직 많이 부족함을 느껴 공부를 더 해야합니다.)   

끝에서도 말하겠지만, 제가 제출하려고 하는 최종 결과물의 핵심은 아래와 같습니다.   

1. train_dataset에서의 y=1 ratio 비율이 제출하려는 test_dataset에서의 비율과 거의 같아야 합니다.

2. 이번 대회의 평가 지표인 macro f1의 값이 높아야 합니다.

3. 당연한 소리이지만 올바른 class를 많이 맞추어야 합니다.



이게 너무 어려웠습니다.   

아직 포스팅을 작성하고 있는 시점이라서, 해결은 못했지만...     

어떤 현상이 벌어지냐면요..    

fi score가 높으면, y=1 ratio의 값이 너무나도 튀게 됩니다.   

ex) f1 score 0.666, y=1 ratio  0.5   

<img src="../../images/2022-02-05/1.png">

f1 score가 precision과 recall에 영향을 받고,

Macro f1은 f1 score에 영향을 받습니다.    

그러면 precision과 recall의 값이 높으면 되는데, 단순히 이 값만을 높이게 되면,    

y=1 ratio가 튀게되어서 결과적으로 leaderboard에서도 낮은 점수를 기록하게 되었습니다.    

(참고로 y=1 ratio가 튀게되어서 점수가 낮아진다고 추측하고있는것 뿐입니다.)      

밑에서마저 얘기해보겠습니다.   








```python
X_train_dataset4
```


  <div id="df-1b2092a9-5286-4373-9353-78d3082fbb4c">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0824</td>
      <td>21000.0</td>
      <td>29.19</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3016</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11773</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>1200.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>18</td>
      <td>37.74</td>
      <td>0.076</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>157.94</td>
      <td>765</td>
      <td>769</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1299</td>
      <td>80000.0</td>
      <td>4.82</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>5722</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21875</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8000</td>
      <td>8000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>269.52</td>
      <td>0.447</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1702.42</td>
      <td>665</td>
      <td>669</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1299</td>
      <td>38000.0</td>
      <td>23.66</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>6511</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31868</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>5000</td>
      <td>5000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>7</td>
      <td>168.45</td>
      <td>0.880</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1066.64</td>
      <td>670</td>
      <td>674</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1367</td>
      <td>100000.0</td>
      <td>16.27</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>6849</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>326049</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>15000</td>
      <td>15000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>12</td>
      <td>510.27</td>
      <td>0.457</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1256.24</td>
      <td>680</td>
      <td>684</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.1269</td>
      <td>30000.0</td>
      <td>25.28</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>8197</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>2506</td>
      <td>8840</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10000</td>
      <td>10000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>335.45</td>
      <td>0.416</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>871.04</td>
      <td>660</td>
      <td>664</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>99995</th>
      <td>0.1757</td>
      <td>65000.0</td>
      <td>17.67</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>11255</td>
      <td>21</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>26570</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>20000</td>
      <td>20000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>13</td>
      <td>718.75</td>
      <td>0.780</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5373.29</td>
      <td>660</td>
      <td>664</td>
    </tr>
    <tr>
      <th>99996</th>
      <td>0.0890</td>
      <td>65000.0</td>
      <td>2.88</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2105</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6138</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6000</td>
      <td>6000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>7</td>
      <td>190.52</td>
      <td>0.120</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>835.66</td>
      <td>765</td>
      <td>769</td>
    </tr>
    <tr>
      <th>99997</th>
      <td>0.1349</td>
      <td>46000.0</td>
      <td>32.12</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>8998</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>96531</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6400</td>
      <td>6400.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>19</td>
      <td>217.16</td>
      <td>0.643</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1261.67</td>
      <td>665</td>
      <td>669</td>
    </tr>
    <tr>
      <th>99998</th>
      <td>0.2115</td>
      <td>31000.0</td>
      <td>4.53</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3875</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3875</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5500</td>
      <td>5500.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>3</td>
      <td>207.64</td>
      <td>0.731</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1357.69</td>
      <td>710</td>
      <td>714</td>
    </tr>
    <tr>
      <th>99999</th>
      <td>0.1599</td>
      <td>125000.0</td>
      <td>33.33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>34580</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>422626</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>33125</td>
      <td>33125.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>19</td>
      <td>1164.42</td>
      <td>0.499</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8882.58</td>
      <td>690</td>
      <td>694</td>
    </tr>
  </tbody>
</table>
<p>80000 rows × 75 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1b2092a9-5286-4373-9353-78d3082fbb4c')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1b2092a9-5286-4373-9353-78d3082fbb4c button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1b2092a9-5286-4373-9353-78d3082fbb4c');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  


# Single Model(XGBoost)

## XGBClassifier의 하이퍼 파라미터 설명

- **LINK** : https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier

- **max_depth=3** : 디시전 트리의 최대 깊이

- **learning_rate=0.1** : 0과 1 사이의 값을 가지는 부스팅에 대한 학습률(eta). 매 부스팅 단계 이후 새로이 추가된 가중치는 이 파라미터로 조정된다. 이 값이 낮을수록 보수적이며, 수렴에 필요한 더 많은 디시전 트리가 필요하다.

- **n_estimators=100** : 라운드의 횟수 또는 부스팅된 디시전 트리의 개수

- **silent=True** : 부스팅의 수행 동안 메시지를 출력할지에 대한 여부

- **objective="reg:linear"** : 모델이 학습할 작업의 분류, 미리 정의된 작업은 문자열로 지정이 가능하지만, 그렇지 않은 경우 콜러블callable을 만들어서 지정할 수도 있다.

- **booster="gbtree"** : 'gbtree', 'gblinear', 'dart'일 수 있다. 'dart'는 드롭아웃(dropout)이라는 특성을 추가한다(과적합 방지를 위해 무작위로 디시전 트리를 선택해 제거(드롭)한다). 'gblinear'는 정규화된 선형 모델을 만든다(디시전 트리가 아니라 라소 회귀와 유사하다).

- **nthread=None** : 더 이상 사용되지 않는다.

- **n_jobs** : 사용할 스레드의 개수

- **gamma=0** : 노드 분할에 필요한 최소 손실 감소

- **min_child_weight=1** : 자식 노드 생성에 필요한 헤시안(hessian) 합의 최솟값

- **max_delta_step=0** : 보다 보수적으로 갱신을 수행하도록 만드는 값. 불균형 범주의 데이터셋에 대해서는 1부터 10까지의 값으로 설정한다.

- **subsample=1** : 부스팅에 사용할 샘플의 비율

- **colsample_bytree=1** : 부스팅에 사용할 특징 열의 비율

- **colsample_bylevel=1** : 각 디시전 트리의 수준별 사용할 특징 열의 비율

- **colsample_bynode=1** : 각 디시전 트리의 노드별 사용할 특징 열의 비율

- **reg_alpha=0** : L1 정규화(가중치의 평균). 이 값이 클수록 보수적이게 된다.

- **reg_lambda=1** : L2 정규화(가중치의 제곱근). 이 값이 클수록 보수적이게 된다.

- **base_score=.5** : 초기 편향치(bias)

- **seed=None** : 더 이상 사용되지 않는다.

- **random_state=0** : 난수 생성 시드

- **missing=None** : 누락된 데이터가 해석될 값. None은 np.nan을 의미한다.

- **importance_type='gain'** : 특징 중요도의 유형. 'gain', 'weight', 'cover', 'total_gain', 'total_cover'로 설정될 수 있다.



```python
# # 하이퍼 파라미터 튜닝
# xgb_clf = xgboost.XGBClassifier()

# param_grid = {'max_depth':[5,10],
#               'n_estimators':[50, 100],
#               }

# cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2)

# grid_search = GridSearchCV(estimator=xgb_clf,
#                            param_grid=param_grid, 
#                            n_jobs=-1,
#                            cv=cv,
#                            scoring='accuracy', 
#                            error_score=0) 

# results=grid_search.fit(X_train_dataset1, y_train_dataset1)

# results.best_params_
```


```python
# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# X_train = sc.fit_transform(X_train)
# X_valid = sc.transform(X_valid)
```

# XGBoost hyperparameter tuning


hyperparameter tuning을 진행하면서, plot이 사라진게 있습니다.   

시간이 오래걸려서 중간에 멈춰버린게 많았거든요.ㅜㅜ    

그래도 값들은 직접 구해본것들입니다.!!!    

이 부분은 "튜닝을 이런식으로 했구나" 정도만 보고 지나가셔도 될것같습니다.



```python
tuning_dataset = df[:80000]
tuning_testset = df[80000:]
idx = [ i for i in range(80000)]
tuning_dataset["Id"] = idx
tuning_dataset
```


  <div id="df-80e5c14c-09c1-415a-8280-f287ff8131be">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
      <th>depvar</th>
      <th>Id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0824</td>
      <td>21000.0</td>
      <td>29.19</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3016</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11773</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1200</td>
      <td>1200.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>18</td>
      <td>37.74</td>
      <td>0.076</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>157.94</td>
      <td>765</td>
      <td>769</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1299</td>
      <td>80000.0</td>
      <td>4.82</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>5722</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21875</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8000</td>
      <td>8000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>269.52</td>
      <td>0.447</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1702.42</td>
      <td>665</td>
      <td>669</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1299</td>
      <td>38000.0</td>
      <td>23.66</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>6511</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31868</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>5000</td>
      <td>5000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>7</td>
      <td>168.45</td>
      <td>0.880</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1066.64</td>
      <td>670</td>
      <td>674</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1367</td>
      <td>100000.0</td>
      <td>16.27</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>6849</td>
      <td>30</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>326049</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>15000</td>
      <td>15000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>12</td>
      <td>510.27</td>
      <td>0.457</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1256.24</td>
      <td>680</td>
      <td>684</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.1269</td>
      <td>30000.0</td>
      <td>25.28</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>8197</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>2506</td>
      <td>8840</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10000</td>
      <td>10000.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>8</td>
      <td>335.45</td>
      <td>0.416</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>871.04</td>
      <td>660</td>
      <td>664</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>79995</th>
      <td>0.1333</td>
      <td>114480.0</td>
      <td>7.81</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>5680</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>108</td>
      <td>340793</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10800</td>
      <td>10800.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>18</td>
      <td>365.62</td>
      <td>0.458</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1868.28</td>
      <td>670</td>
      <td>674</td>
      <td>0</td>
      <td>79995</td>
    </tr>
    <tr>
      <th>79996</th>
      <td>0.0699</td>
      <td>88800.0</td>
      <td>7.91</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9617</td>
      <td>11</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>230979</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9800</td>
      <td>9800.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>4</td>
      <td>302.56</td>
      <td>0.641</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>725</td>
      <td>729</td>
      <td>1</td>
      <td>79996</td>
    </tr>
    <tr>
      <th>79997</th>
      <td>0.1299</td>
      <td>32000.0</td>
      <td>5.96</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>952</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4410</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5600</td>
      <td>5600.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>4</td>
      <td>188.66</td>
      <td>0.090</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1188.09</td>
      <td>745</td>
      <td>749</td>
      <td>0</td>
      <td>79997</td>
    </tr>
    <tr>
      <th>79998</th>
      <td>0.1333</td>
      <td>60000.0</td>
      <td>18.58</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31287</td>
      <td>14</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35108</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>27600</td>
      <td>27600.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>7</td>
      <td>934.35</td>
      <td>0.711</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6015.99</td>
      <td>705</td>
      <td>709</td>
      <td>0</td>
      <td>79998</td>
    </tr>
    <tr>
      <th>79999</th>
      <td>0.2099</td>
      <td>40000.0</td>
      <td>22.92</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2777</td>
      <td>22</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>23758</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>11125</td>
      <td>11125.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>10</td>
      <td>300.91</td>
      <td>0.237</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6743.82</td>
      <td>705</td>
      <td>709</td>
      <td>0</td>
      <td>79999</td>
    </tr>
  </tbody>
</table>
<p>80000 rows × 77 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-80e5c14c-09c1-415a-8280-f287ff8131be')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-80e5c14c-09c1-415a-8280-f287ff8131be button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-80e5c14c-09c1-415a-8280-f287ff8131be');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  



```python
def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=100):
   
    # get new n_estimator
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgboost.DMatrix(dtrain[predictors].values, label=dtrain[target].values)
        cvresult = xgboost.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
                          metrics='error', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
        print(alg)
    
    # Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain['depvar'], eval_metric='error')
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
        
    #Print model report:
    print("\nModel Report")
    print("Training Accuracy : %.4g" % metrics.accuracy_score(dtrain['depvar'].values, dtrain_predictions))
```


```python
target = 'depvar'
IDcol = 'Id'
```


```python
# n_estimators를 1000으로 두면 왜 training accuracy가 더 떨어질까? 100보다.
predictors = [x for x in tuning_dataset.columns if x not in [target, IDcol]]
model1 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 1000,
                                max_depth = 5,
                                min_child_weight = 1,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205                                
                                )
modelfit(model1, tuning_dataset, predictors)
```

## n_estimators fix



```python

```

<pre>
1.0.2
</pre>

```python
## GPU 쓰고 싶다.
# tree_method='gpu_hist',
# predictor='gpu_predictor'


cv_scores = list()
estimator_list = [i for i in range(100, 1000, 50)]
for i in tqdm(range(100, 1000, 50)):
  xgbc = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = i,
                                max_depth = 5,
                                min_child_weight = 1,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor'
                                )
  score = cross_val_score(xgbc, X_train_dataset1, y_train_dataset1, cv=5, scoring="f1_macro").mean()
  cv_scores.append(score)

  best_e = [estimator_list[i] for i in range(len(cv_scores)) if cv_scores[i] == np.max(cv_scores)]
```


```python
plt.figure(figsize=(20,10))
plt.plot(estimator_list, cv_scores, marker='o', linestyle='dashed')
plt.axvline(best_e[0], color='r', linestyle= '--', linewidth=2)
```


```python
print(f"optimizer tree count : {(cv_scores.index(max(cv_scores)))} ")
# 350
print(f"Train set에 대한 성능 : {max(cv_scores):.4f} ")
```

## Max depth



```python

cv_scores = list()
max_depth_list = [5, 7, 9]
for i in tqdm(max_depth_list):
  xgbc = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 350,
                                max_depth = i,
                                min_child_weight = 1,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')
  score = cross_val_score(xgbc, X_train_dataset1, y_train_dataset1, cv=5, scoring="f1_macro").mean()
  cv_scores.append(score)

  best_e = [max_depth_list[i] for i in range(len(cv_scores)) if cv_scores[i] == np.max(cv_scores)]
```


```python
plt.figure(figsize=(20,10))
plt.plot(max_depth_list, cv_scores, marker='o', linestyle='dashed')
plt.axvline(best_e[0], color='r', linestyle= '--', linewidth=2)
```


```python
print(f"optimizer tree count : {(cv_scores.index(max(cv_scores)))} ")
# 9
print(f"Train set에 대한 성능 : {max(cv_scores):.4f} ")
```

## gamma



```python

cv_scores = list()
gamma_list = [0.0, 0.1, 0.2, 0.3, 0.4]
for i in tqdm(gamma_list):
  xgbc = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 350,
                                max_depth = 9,
                                min_child_weight = 1,
                                gamma = i,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                eval_metric='error',
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')
  score = cross_val_score(xgbc, X_train_dataset1, y_train_dataset1, cv=5, scoring="accuracy").mean()
  cv_scores.append(score)

  best_e = [gamma_list[i] for i in range(len(cv_scores)) if cv_scores[i] == np.max(cv_scores)]
```

<pre>
100%|██████████| 5/5 [04:19<00:00, 51.92s/it]
</pre>

```python
plt.figure(figsize=(20,10))
plt.plot(gamma_list, cv_scores, marker='o', linestyle='dashed')
plt.axvline(best_e[0], color='r', linestyle= '--', linewidth=2)
```

<pre>
<matplotlib.lines.Line2D at 0x7f5c64528c90>
</pre>
<img src="data:image/png;base64,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"/>


```python
print(f"optimizer tree count : {(cv_scores.index(max(cv_scores)))} ")
# 
print(f"Train set에 대한 성능 : {max(cv_scores):.4f} ")
```

<pre>
optimizer tree count : 0 
Train set에 대한 성능 : 0.7508 
</pre>
## min_child_weight



```python

cv_scores = list()
min_child_weight_list = [i for i in range(1,10,2)]
for i in tqdm(range(1,10,2)):
  xgbc = xgboost.XGBClassifier(learning_rate = 0.1,
                                n_estimators = 350,
                                max_depth = 5,
                                min_child_weight = i,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                eval_metric='error',
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')
  score = cross_val_score(xgbc, X_train_dataset1, y_train_dataset1, cv=5, scoring="accuracy").mean()
  cv_scores.append(score)

  best_e = [min_child_weight_list[i] for i in range(len(cv_scores)) if cv_scores[i] == np.max(cv_scores)]
```

<pre>
100%|██████████| 5/5 [01:20<00:00, 16.01s/it]
</pre>

```python
plt.figure(figsize=(20,10))
plt.plot(min_child_weight_list, cv_scores, marker='o', linestyle='dashed')
plt.axvline(best_e[0], color='r', linestyle= '--', linewidth=2)
```

<pre>
<matplotlib.lines.Line2D at 0x7f5c4b3b71d0>
</pre>
<img src="data:image/png;base64,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"/>


```python
print(f"optimizer tree count : {(cv_scores.index(max(cv_scores)))} ")
# 1
print(f"Train set에 대한 성능 : {max(cv_scores):.4f} ")
```

<pre>
optimizer tree count : 0 
Train set에 대한 성능 : 0.7541 
</pre>

```python

```

# XGBoost Model Training


위에서 얻어진 값들은 모델의 학습 파라미터로 주고 있습니다.    





```

tree_method='gpu_hist',

predictor='gpu_predictor'

```

이 부분은 gpu로 학습시킨다는 의미이며, gpu가 없을시 저 부분을 삭제하고 진행하시면 됩니다.    

Train Valid Split 에서 설명한대로 학습을 진행하고 있습니다.






```python
model1 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 350,
                                max_depth = 9,
                                min_child_weight = 1,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')


model2 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 1000,
                                max_depth = 5,
                                min_child_weight = 10,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')

model3 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 1000,
                                max_depth = 5,
                                min_child_weight = 10,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')

model4 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 1000,
                                max_depth = 5,
                                min_child_weight = 10,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')

model5 = xgboost.XGBClassifier(learning_rate = 0.01,
                                n_estimators = 1000,
                                max_depth = 5,
                                min_child_weight = 10,
                                gamma = 0,
                                subsample = 0.8,
                                colsample_bytree = 0.8,
                                objective="binary:logistic",
                                nthread= -1,
                                scale_pos_weight = 1,
                                seed=220205,
                                tree_method='gpu_hist',
                                predictor='gpu_predictor')

model1.fit(X_train_dataset1, y_train_dataset1)
model2.fit(X_train_dataset2, y_train_dataset2)
model3.fit(X_train_dataset3, y_train_dataset3)
model4.fit(X_train_dataset4, y_train_dataset4)
model5.fit(X_train_dataset5, y_train_dataset5)

```

<pre>
XGBClassifier(colsample_bytree=0.8, learning_rate=0.01, max_depth=5,
              min_child_weight=10, n_estimators=1000, nthread=-1,
              predictor='gpu_predictor', seed=220205, subsample=0.8,
              tree_method='gpu_hist')
</pre>

```python
y_pred1 = model1.predict(X_train5)
y_pred2 = model2.predict(X_train4)
y_pred3 = model3.predict(X_train3)
y_pred4 = model4.predict(X_train2)
y_pred5 = model5.predict(X_train1)

# y_valid가 0 또는 1일 확률 출력
y_prob1 = model1.predict_proba(X_train5)
y_prob2 = model2.predict_proba(X_train4)
y_prob3 = model3.predict_proba(X_train3)
y_prob4 = model4.predict_proba(X_train2)
y_prob5 = model5.predict_proba(X_train1)

```


```python
# 1로 예측된 y_valid 갯수 및 비율 출력
print("thr > 0.5")
print(f" model1의 예측비율 : {y_pred1.sum() / len(y_pred1)}")
print(f" model2의 예측비율 : {y_pred2.sum() / len(y_pred2)}")
print(f" model3의 예측비율 : {y_pred3.sum() / len(y_pred3)}")
print(f" model4의 예측비율 : {y_pred4.sum() / len(y_pred4)}")
print(f" model5의 예측비율 : {y_pred5.sum() / len(y_pred5)}")

# thr 0.3보다 큰 경우
thr = 0.38
print(f"thr > {thr}")
pred1 = (y_prob1[:,1] >= thr).astype(np.int64)
pred2 = (y_prob2[:,1] >= thr).astype(np.int64)
pred3 = (y_prob3[:,1] >= thr).astype(np.int64)
pred4 = (y_prob4[:,1] >= thr).astype(np.int64)
pred5 = (y_prob5[:,1] >= thr).astype(np.int64)
print(f" model1의 예측비율 : {pred1.sum() / len(pred1)}")
print(f" model1의 예측비율 : {pred2.sum() / len(pred2)}")
print(f" model1의 예측비율 : {pred3.sum() / len(pred3)}")
print(f" model1의 예측비율 : {pred4.sum() / len(pred4)}")
print(f" model1의 예측비율 : {pred5.sum() / len(pred5)}")
```

<pre>
thr > 0.5
 model1의 예측비율 : 0.2039
 model2의 예측비율 : 0.2112
 model3의 예측비율 : 0.2054
 model4의 예측비율 : 0.21025
 model5의 예측비율 : 0.2036
thr > 0.38
 model1의 예측비율 : 0.3277
 model1의 예측비율 : 0.3358
 model1의 예측비율 : 0.32705
 model1의 예측비율 : 0.33205
 model1의 예측비율 : 0.32595
</pre>
여기에서 보면 predict()로 구하게 되면 thr값이 0.5가 default여서 y=1 ratio의 값이 train_dataset과 차이가 많이 나게 됩니다.   

그래서 predict_proba()로 구해서 thr의 값을 수정해주는 작업을 진행해 주어야합니다.   

사실 thr 값을 제가 이렇게 바꾸는게, 기껏 모델 학습시켜놓은것을 망친다는 생각도 했으며,   

모델을 학습시킬대 thr값을 제가 변경해주면서 학습시키는것이 가능한가?   

이 부분에 대한 공부도 필요합니다.    

docs도 뒤져보고 있는데, 능력부족으로 파악하지 못했습니다.   

직접 구현해야 하는것인지 ㅜㅜ   



```python
# 평가 함수 정의
def get_clf_eval(y_actual, y_pred):
    accuracy = accuracy_score(y_actual, y_pred)
    precision = precision_score(y_actual, y_pred)
    recall = recall_score(y_actual, y_pred)
    AUC = roc_auc_score(y_actual, y_pred)
    F1 = f1_score(y_actual, y_pred)
    print('\naccuracy: {:.4f}'.format(accuracy))
    print('precision: {:.4f}'.format(precision))
    print('recall: {:.4f}'.format(recall))
    print('AUC: {:.4f}'.format(AUC))
    print('F1: {:.4f}'.format(F1))
    
    # sns.heatmap(confusion_matrix(y_actual, y_pred), annot=True, fmt='d', cmap='YlGnBu')
```


```python
# xgboost 성능 확인
get_clf_eval(y_train5, pred1)
get_clf_eval(y_train4, pred2)
get_clf_eval(y_train3, pred3)
get_clf_eval(y_train2, pred4)
get_clf_eval(y_train1, pred5)
```

<pre>

accuracy: 0.7500
precision: 0.6038
recall: 0.6221
AUC: 0.7158
F1: 0.6128

accuracy: 0.7428
precision: 0.6127
recall: 0.6181
AUC: 0.7116
F1: 0.6154

accuracy: 0.7459
precision: 0.6149
recall: 0.6107
AUC: 0.7115
F1: 0.6128

accuracy: 0.7456
precision: 0.6026
recall: 0.6203
AUC: 0.7127
F1: 0.6113

accuracy: 0.7435
precision: 0.6059
recall: 0.6066
AUC: 0.7081
F1: 0.6062
</pre>
만족스러운 결과물은 아닙니다...    

MNIST나 sklearn에서 제공하는 다른 데이터들에서는 항상 학습시키면 90이라는 숫자가 넘게 나왔는데,    

train_dataset에서 학습을 시키는데도 불구하고 80을 못넘겼습니다...   

test에서는 얼마나 더 떨어질지 ㅠㅠ   


# Randomforest hyperparameter tuning



```python

```


```python

```

# Randomforest Model Training


# Submission



```python
# 제출 양식 다운로드
submit = pd.read_csv('/content/drive/MyDrive/22-01-28/sample_submission.csv')

# prediction 수행
df_test = pd.read_csv('/content/drive/MyDrive/22-01-28/test.csv')
```


```python
test = df_test.drop('ID', axis=1)
```


```python
test
```


  <div id="df-9ec21fbf-ae9b-405c-a6c1-b2f05129b684">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>int_rate</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>inq_last_6mths</th>
      <th>pub_rec</th>
      <th>revol_bal</th>
      <th>total_acc</th>
      <th>collections_12_mths_ex_med</th>
      <th>acc_now_delinq</th>
      <th>tot_coll_amt</th>
      <th>tot_cur_bal</th>
      <th>chargeoff_within_12_mths</th>
      <th>delinq_amnt</th>
      <th>tax_liens</th>
      <th>emp_length1</th>
      <th>emp_length2</th>
      <th>emp_length3</th>
      <th>emp_length4</th>
      <th>emp_length5</th>
      <th>emp_length6</th>
      <th>emp_length7</th>
      <th>emp_length8</th>
      <th>emp_length9</th>
      <th>emp_length10</th>
      <th>emp_length11</th>
      <th>emp_length12</th>
      <th>home_ownership1</th>
      <th>home_ownership2</th>
      <th>home_ownership3</th>
      <th>home_ownership4</th>
      <th>home_ownership5</th>
      <th>home_ownership6</th>
      <th>verification_status1</th>
      <th>verification_status2</th>
      <th>verification_status3</th>
      <th>purpose1</th>
      <th>purpose2</th>
      <th>purpose3</th>
      <th>purpose4</th>
      <th>purpose5</th>
      <th>purpose6</th>
      <th>purpose7</th>
      <th>purpose8</th>
      <th>purpose9</th>
      <th>purpose10</th>
      <th>purpose11</th>
      <th>purpose12</th>
      <th>purpose13</th>
      <th>purpose14</th>
      <th>initial_list_status1</th>
      <th>initial_list_status2</th>
      <th>mths_since_last_delinq1</th>
      <th>mths_since_last_delinq2</th>
      <th>mths_since_last_delinq3</th>
      <th>mths_since_last_delinq4</th>
      <th>mths_since_last_delinq5</th>
      <th>mths_since_last_delinq6</th>
      <th>mths_since_last_delinq7</th>
      <th>mths_since_last_delinq8</th>
      <th>mths_since_last_delinq9</th>
      <th>mths_since_last_delinq10</th>
      <th>mths_since_last_delinq11</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>total_rec_late_fee</th>
      <th>term1</th>
      <th>open_acc</th>
      <th>installment</th>
      <th>revol_util</th>
      <th>out_prncp</th>
      <th>out_prncp_inv</th>
      <th>total_rec_int</th>
      <th>fico_range_low</th>
      <th>fico_range_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1449</td>
      <td>16380.0</td>
      <td>26.08</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3486</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9214</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8100</td>
      <td>8100.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>4</td>
      <td>278.78</td>
      <td>0.311</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>460.40</td>
      <td>700</td>
      <td>704</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.1899</td>
      <td>65000.0</td>
      <td>13.97</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>25305</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>115612</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>20000</td>
      <td>20000.0</td>
      <td>0.00</td>
      <td>0</td>
      <td>10</td>
      <td>518.71</td>
      <td>0.885</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4866.68</td>
      <td>675</td>
      <td>679</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.1049</td>
      <td>53000.0</td>
      <td>23.28</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10910</td>
      <td>21</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>33017</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10000</td>
      <td>10000.0</td>
      <td>16.25</td>
      <td>1</td>
      <td>7</td>
      <td>324.98</td>
      <td>0.580</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1451.06</td>
      <td>675</td>
      <td>679</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1757</td>
      <td>71800.0</td>
      <td>30.32</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>42423</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>152515</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>27200</td>
      <td>27200.0</td>
      <td>0.00</td>
      <td>0</td>
      <td>16</td>
      <td>684.36</td>
      <td>0.701</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7068.11</td>
      <td>665</td>
      <td>669</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2020</td>
      <td>50000.0</td>
      <td>25.61</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>21703</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>135282</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>22000</td>
      <td>22000.0</td>
      <td>0.00</td>
      <td>0</td>
      <td>13</td>
      <td>585.32</td>
      <td>0.622</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7754.20</td>
      <td>710</td>
      <td>714</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>35811</th>
      <td>0.1049</td>
      <td>110000.0</td>
      <td>9.02</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>8991</td>
      <td>33</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>17468</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>12000</td>
      <td>12000.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>13</td>
      <td>389.98</td>
      <td>0.346</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1406.13</td>
      <td>665</td>
      <td>669</td>
    </tr>
    <tr>
      <th>35812</th>
      <td>0.0824</td>
      <td>45000.0</td>
      <td>32.56</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>20966</td>
      <td>29</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>24802</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5000</td>
      <td>5000.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>9</td>
      <td>157.24</td>
      <td>0.690</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>495.70</td>
      <td>705</td>
      <td>709</td>
    </tr>
    <tr>
      <th>35813</th>
      <td>0.1368</td>
      <td>49000.0</td>
      <td>17.60</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5597</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>20691</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6000</td>
      <td>6000.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>7</td>
      <td>204.14</td>
      <td>0.333</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1355.06</td>
      <td>705</td>
      <td>709</td>
    </tr>
    <tr>
      <th>35814</th>
      <td>0.1139</td>
      <td>84852.0</td>
      <td>9.96</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>7184</td>
      <td>13</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>31396</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10000</td>
      <td>10000.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>8</td>
      <td>329.24</td>
      <td>0.352</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1615.15</td>
      <td>725</td>
      <td>729</td>
    </tr>
    <tr>
      <th>35815</th>
      <td>0.1274</td>
      <td>60000.0</td>
      <td>3.64</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>77</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>1468</td>
      <td>178885</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1500</td>
      <td>1500.0</td>
      <td>0.00</td>
      <td>1</td>
      <td>3</td>
      <td>50.36</td>
      <td>0.770</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>311.08</td>
      <td>675</td>
      <td>679</td>
    </tr>
  </tbody>
</table>
<p>35816 rows × 75 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-9ec21fbf-ae9b-405c-a6c1-b2f05129b684')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-9ec21fbf-ae9b-405c-a6c1-b2f05129b684 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-9ec21fbf-ae9b-405c-a6c1-b2f05129b684');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  



```python
# out1 = model1.predict(test)
# out2 = model2.predict(test)
# out3 = model3.predict(test)
# out4 = model4.predict(test)
# out5 = model5.predict(test)


prob1 = model1.predict_proba(test)
out1 = (prob1[:,1] >= thr).astype(np.int64)

prob2 = model2.predict_proba(test)
out2 = (prob2[:,1] >= thr).astype(np.int64)

prob3 = model3.predict_proba(test)
out3 = (prob3[:,1] >= thr).astype(np.int64)

prob4 = model4.predict_proba(test)
out4 = (prob4[:,1] >= thr).astype(np.int64)

prob5 = model5.predict_proba(test)
out5 = (prob5[:,1] >= thr).astype(np.int64)




print(out1.sum() / len(out1))
print(out2.sum() / len(out2))
print(out3.sum() / len(out3))
print(out4.sum() / len(out4))
print(out5.sum() / len(out5))






```

<pre>
0.3358554835827563
0.334682823319187
0.332449184721912
0.3370002233638597
0.3345432209068573
</pre>

```python
result = []
for i in range(len(test)):
  if 3 == (out1[i] + out2[i] + out3[i] + out4[i] + out5[i]):
    result.append(1)
  elif 4 == (out1[i] + out2[i] + out3[i] + out4[i] + out5[i]):
    result.append(1)
  elif 5 == (out1[i] + out2[i] + out3[i] + out4[i] + out5[i]):
    result.append(1)
  else:
    result.append(0)

    
  
```


```python
print(f" 최종 결과물!!! ")
print(f" {sum(result) / len(result)} ")
print(len(result))
print(result.count(1))

submit["answer"] = result  
```

<pre>
 최종 결과물!!! 
 0.3345153004243913 
35816
11981
</pre>

```python
# 제출 파일 저장
submit.to_csv('/content/drive/MyDrive/22-02-05/2022-02-05-14.csv', index=False)
```

y=1 ratio가 우선순위 1번이며,   

f1 score가 우선순위 2번으로,    

모델을 수정하고, 학습시켰습니다.   

대회 마감일날, 최종 제출할때도 위의 2개의 원칙을 지켜서 제출해보려 합니다.(공부를 해서 뭔가를 알아내기 전까지는 그대로일듯 합니다.)   

아래의 주석들은 제가 제출한 파일들의 점수와 hyperparameter들입니다.    



끝으로..    

공부할게 참 많습니다.!!!!



```python
##########################################3
# CSV file 설명
# 2022-02-05-1 : hyperparameter tuning 하지 않았고, predict(thrhold:0.5)로만 하였음.
# 2022-02-05-2 : hyperparameter tuning 하지 않았고, predict(thrhold:0.42)로만 하였음.
# 2022-02-05-3 : hyperparameter tuning 한 결과, predict(thrhold:0.42)로만 하였음.(어떻게 튜닝했는데 더 떨어지지?)
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-4 : hyperparameter tuning 한 결과, predict(thrhold:0.38)로만 하였음.(최고기록: 0.717169)
# 2022-02-05-5 : hyperparameter tuning 한 결과, predict(thrhold:0.35)로 하였음. 0.35로 했더니 ratio of label는 망가졌으나(0.39), fi score가 높음! 0.637~0.638
# 2022-02-05-6 : hyperparameter tuning 한 결과, predict(thrhold:0.35)로 하였음.
# score는 713982
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 1000,
#                                 max_depth = 5,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 eval_metric='error',
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-7 : hyperparameter tuning 한 결과, predict(thrhold:0.38)로 하였음.
# score는 713330
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 1000,
#                                 max_depth = 5,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 eval_metric='error',
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-8 : hyperparameter tuning 한 결과, predict(thrhold:0.4)로 하였음.
# score는 713574
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 eval_metric='error',
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-9 : hyperparameter tuning 한 결과, predict(thrhold:0.405)로 하였음.
# score는 712255
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-10 : hyperparameter tuning 한 결과, predict(thrhold:0.4)로 하였음.
# score는 713574
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-11 : hyperparameter tuning 한 결과, predict(thrhold:0.38)로 하였음.
# score는 716175
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-12 : hyperparameter tuning 한 결과, predict(thrhold:0.36)로 하였음.
# score는 716011
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 350,
#                                 max_depth = 9,
#                                 min_child_weight = 1,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
# 2022-02-05-13 : hyperparameter tuning 한 결과, predict(thrhold:0.38-0.4)로 하였음.
# score는 713264
# model1만 다르게 해본거.
# 2022-02-05-14 : hyperparameter tuning 한 결과, predict(thrhold:0.38)로 하였음.
# score는 713171
# xgboost.XGBClassifier(learning_rate = 0.01,
#                                 n_estimators = 1000,
#                                 max_depth = 5,
#                                 min_child_weight = 10,
#                                 gamma = 0,
#                                 subsample = 0.8,
#                                 colsample_bytree = 0.8,
#                                 objective="binary:logistic",
#                                 nthread= -1,
#                                 scale_pos_weight = 1,
#                                 seed=220205,
#                                 tree_method='gpu_hist',
#                                 predictor='gpu_predictor')
```


```python

```
